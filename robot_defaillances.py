import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import IsolationForest
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split

# =============================================================================
# CHARGEMENT ET EXPLORATION DES DONN√âES
# =============================================================================

print("="*70)
print("EXERCICE 3 - D√âTECTION DE D√âFAILLANCES D'UN ROBOT")
print("="*70)

# 1. Charger le fichier Robot.csv
# üö® Correction : Utilisation du chargement standard pour les environnements locaux
try:
    df = pd.read_csv("Robot.csv")
except FileNotFoundError:
    print("Erreur fatale : Le fichier 'Robot.csv' est introuvable.")
    print("Assurez-vous que le fichier est dans le m√™me r√©pertoire que le script.")
    import sys
    sys.exit() # Arr√™te le script si le fichier n'est pas trouv√©

print("\nüìä Aper√ßu des donn√©es:")
print(df.head())
print(f"\nDimensions: {df.shape}")
print(f"Colonnes: {list(df.columns)}")

# 2. Afficher le nombre de cycles normaux et d√©faillants
print("\nüìà R√©partition des cycles:")
print(df["Cycle_Normal"].value_counts())
print(f"\nPourcentage de d√©faillances: {(df['Cycle_Normal']==0).sum()/len(df)*100:.2f}%")

# 3. Pairplot avec cycles d√©faillants en rouge
print("\nüé® G√©n√©ration du pairplot...")
sns.pairplot(df, hue="Cycle_Normal", palette={1:"green", 0:"red"})
plt.suptitle("Pairplot - Cycles normaux (vert) vs d√©faillants (rouge)", y=1.01)
plt.show()

# =============================================================================
# PARTIE 1 - APPRENTISSAGE NON SUPERVIS√â : ISOLATION FOREST
# =============================================================================

print("\n" + "="*70)
print("PARTIE 1 - ISOLATION FOREST (Non supervis√©)")
print("="*70)

# A. Copier toutes les colonnes sauf la derni√®re dans X
X = df.iloc[:, :-1]
print("\nüì¶ Caract√©ristiques (X):")
print(X.info())

# B. Isolation Forest avec 2% de contamination
print("\nüå≤ Entra√Ænement de l'Isolation Forest...")
model = IsolationForest(contamination=0.02, random_state=42)
model.fit(X)

pred = model.predict(X)
print(f"Pr√©dictions: {len(pred[pred==-1])} anomalies d√©tect√©es")

# C. Tableau crois√© pour √©valuer les performances
temp = pd.DataFrame()
temp["IF_pred"] = pred
temp["Cycle_Normal"] = df["Cycle_Normal"]

print("\nüìä Matrice de confusion (Isolation Forest):")
confusion = pd.crosstab(temp["Cycle_Normal"], temp["IF_pred"],
                        rownames=['R√©el'], colnames=['Pr√©dit'])
print(confusion)

# Calcul des m√©triques (S√©curis√©)
vrais_negatifs = confusion.loc[0, -1] if -1 in confusion.columns and 0 in confusion.index else 0
faux_positifs = confusion.loc[1, -1] if -1 in confusion.columns and 1 in confusion.index else 0
faux_negatifs = confusion.loc[0, 1] if 1 in confusion.columns and 0 in confusion.index else 0
vrais_positifs = confusion.loc[1, 1] if 1 in confusion.columns and 1 in confusion.index else 0

print(f"\n‚úÖ Vrais positifs (normaux d√©tect√©s): {vrais_positifs}")
print(f"‚úÖ Vrais n√©gatifs (d√©fauts d√©tect√©s): {vrais_negatifs}")
print(f"‚ùå Faux positifs (normaux vus comme d√©fauts): {faux_positifs}")
print(f"‚ùå Faux n√©gatifs (d√©fauts non d√©tect√©s): {faux_negatifs}")

# D. Ajouter la colonne des anomalies
tempDF = X.copy()
tempDF["Anomalie"] = pred
print("\nüìã Dataframe avec anomalies:")
print(tempDF.head())

# E. Afficher les lignes contenant des anomalies
anomalies = tempDF[tempDF["Anomalie"] == -1]
print(f"\nüîç {len(anomalies)} anomalies d√©tect√©es:")
print(anomalies)

print("\nüí° Conclusion Isolation Forest:")
print("   - M√©thode non supervis√©e : n'utilise pas l'√©tiquette 'Cycle_Normal'")

# =============================================================================
# PARTIE 2 - APPRENTISSAGE SUPERVIS√â : ARBRE DE D√âCISION (AVEC SPLIT TRAIN/TEST)
# =============================================================================

print("\n" + "="*70)
print("PARTIE 2 - ARBRE DE D√âCISION (Supervis√© avec Train/Test Split)")
print("="*70)

# B. D√©terminer les entr√©es (X) et la sortie (y)
X_tree = df.drop(columns=["Cycle_Normal"])
y_tree = df["Cycle_Normal"]

# üì¢ S√©paration des donn√©es en ensembles d'entra√Ænement (Train) et de test (Test)
X_train, X_test, y_train, y_test = train_test_split(
    X_tree, y_tree, test_size=0.3, random_state=42, stratify=y_tree
)

print(f"\nüì• Entr√©es (X_train): {X_train.shape[0]} exemples d'entra√Ænement")
print(f"üì• Entr√©es (X_test): {X_test.shape[0]} exemples de test")
print(f"üì§ Sortie (y): Classes: {y_tree.unique()} (0=D√©faillant, 1=Normal)")


# C. Entra√Æner l'arbre de d√©cision
print("\nüå≥ Entra√Ænement de l'arbre de d√©cision sur l'ensemble d'entra√Ænement...")
tree = DecisionTreeClassifier(max_depth=None, random_state=0)
tree.fit(X_train, y_train)

# D. Nombre de n≈ìuds et Scores
print(f"üìä Nombre de n≈ìuds dans l'arbre: {tree.tree_.node_count}")
print(f"üìè Profondeur de l'arbre: {tree.get_depth()}")

# Score de pr√©cision (√âvalu√© sur les deux ensembles)
score_train = tree.score(X_train, y_train)
score_test = tree.score(X_test, y_test)

print(f"\nüéØ Pr√©cision sur les donn√©es d'entra√Ænement (Train): {score_train*100:.2f}%")
print(f"üéØ Pr√©cision sur les donn√©es de test (Test): {score_test*100:.2f}%")
print("\nüëâ Si le score Train est beaucoup plus √©lev√© que le score Test, il y a surapprentissage (overfitting).")

# E. Afficher l'arbre
print("\nüé® G√©n√©ration de la visualisation de l'arbre...")
plt.figure(figsize=(20, 12))
plot_tree(tree,
          feature_names=X_tree.columns,
          class_names=["D√©faillant", "Normal"],
          filled=True,
          rounded=True,
          fontsize=10)
plt.title("Arbre de d√©cision - D√©tection de d√©faillances (Entra√Æn√© sur 70% des donn√©es)", fontsize=16, pad=20)
plt.tight_layout()
plt.show()

# =============================================================================
# ANALYSE ET COMMENTAIRES
# =============================================================================

print("\n" + "="*70)
print("ANALYSE ET COMMENTAIRES")
print("="*70)

print("\n‚ùì Pourquoi l'arbre utilise X[0], X[1], X[2] ?")
print("   - X[0], X[1], X[2] sont les variables s√©lectionn√©es car elles sont les plus discriminantes.")

print("\n‚ùì Si j'ajoute d'autres valeurs dans Robot.csv, le r√©sultat change-t-il ?")
print("   ‚úÖ OUI, le mod√®le s'adapte aux nouvelles donn√©es d'entra√Ænement.")

print("\nüéì Diff√©rences cl√©s entre les deux m√©thodes:")
print("   - ISOLATION FOREST : Non supervis√©, cherche l'isolement.")
print("   - ARBRE DE D√âCISION : Supervis√©, utilise les √©tiquettes 'Cycle_Normal' pour maximiser la pr√©cision.")

print("\n" + "="*70)
print("FIN DE L'ANALYSE")
print("="*70)